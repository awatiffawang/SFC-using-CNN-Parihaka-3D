{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\", force_remount=True)\n",
    "%cd /content/drive/MyDrive/FYP/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch # credit to @aladdinpersson\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DoubleConv, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 3, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class UNET(nn.Module): \n",
    "    def __init__(\n",
    "        self, in_channels=1, out_channels=6, features=[64, 128, 256, 512]\n",
    "    ):\n",
    "        super(UNET, self).__init__()\n",
    "        self.ups = nn.ModuleList()\n",
    "        self.downs = nn.ModuleList()\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "    \n",
    "        # Downsampling (Encoder-path)\n",
    "        for feature in features:\n",
    "            self.downs.append(DoubleConv(in_channels, feature))\n",
    "            in_channels = feature\n",
    "\n",
    "        # Upsampling (Decoder-path)\n",
    "        for feature in reversed(features):\n",
    "            self.ups.append(nn.ConvTranspose2d(feature*2, feature, kernel_size=2, stride=2,))\n",
    "            self.ups.append(DoubleConv(feature*2, feature))\n",
    "\n",
    "        # Bottle-neck\n",
    "        self.bottleneck = DoubleConv(features[-1], features[-1]*2)\n",
    "\n",
    "        # Final convolution at output layer\n",
    "        self.final_conv = nn.Conv2d(features[0], out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        skip_connections = []\n",
    "\n",
    "        for down in self.downs:\n",
    "            x = down(x)\n",
    "            skip_connections.append(x)\n",
    "            x = self.pool(x)\n",
    "\n",
    "        x = self.bottleneck(x)\n",
    "        skip_connections = skip_connections[::-1]\n",
    "\n",
    "        for idx in range(0, len(self.ups), 2):\n",
    "            x = self.ups[idx](x)\n",
    "            skip_connection = skip_connections[idx//2]\n",
    "\n",
    "            if x.shape != skip_connection.shape: \n",
    "                x = TF.resize(x, size=skip_connection.shape[2:])\n",
    "\n",
    "            concat_skip = torch.cat((skip_connection, x), dim=1)\n",
    "            x = self.ups[idx+1](concat_skip)\n",
    "\n",
    "        return self.final_conv(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "generator.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "def DataGeneratorZXY(\n",
    "    img_data,\n",
    "    label_data,\n",
    "    size\n",
    "):\n",
    "    img, label = [], []\n",
    "\n",
    "    for i in range(len(img_data[0, 0, :])):\n",
    "        I = cv2.resize(img_data[:, :, i], size, interpolation=cv2.INTER_AREA)\n",
    "        L = cv2.resize(label_data[:, :, i], size, interpolation = cv2.INTER_NEAREST)  \n",
    "        img.append(I)\n",
    "        label.append(L)\n",
    " \n",
    "    for i in range(len(img_data[0, :, 0])):\n",
    "        I = cv2.resize(img_data[:, i, :], size, interpolation=cv2.INTER_AREA)\n",
    "        L  = cv2.resize(label_data[:, i, :], size, interpolation = cv2.INTER_NEAREST)\n",
    "        img.append(I)\n",
    "        label.append(L)\n",
    "\n",
    "    return img, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from generator import DataGeneratorZXY\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "\n",
    "class DataGenerator(Dataset):\n",
    "    def __init__(self\n",
    "        ):\n",
    "        self.size = (128, 256)\n",
    "        self.img_data = np.load('data_train.npz', allow_pickle=True, mmap_mode='r')['data']\n",
    "        self.label_data = np.load('labels_train.npz', allow_pickle=True, mmap_mode='r')['labels']\n",
    "        self.img, self.label = DataGeneratorZXY(self.img_data, self.label_data, self.size)\n",
    "        self.transform = transforms.Compose([transforms.ToTensor()])\n",
    "        #img, label = DataGeneatorMultiplePlaneWideAngle(img, label) # generator code not developed yet\n",
    "        #img, label = HorizontalFip(data, label) \n",
    "   \n",
    "    def __len__(self):\n",
    "        return len(self.img)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.transform(self.img[index]), self.transform(self.label[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "early-stopping.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import numpy as np\n",
    "\n",
    "class EarlyStopping: # thanks pytorchtools.py\n",
    "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
    "    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pt', trace_func=print):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "                            Default: 7\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
    "                            Default: False\n",
    "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "                            Default: 0\n",
    "            path (str): Path for the checkpoint to be saved to.\n",
    "                            Default: 'checkpoint.pt'\n",
    "            trace_func (function): trace print function.\n",
    "                            Default: print            \n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "        self.trace_func = trace_func\n",
    "    def __call__(self, val_loss, model):\n",
    "\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        '''Saves model when validation loss decrease.'''\n",
    "        if self.verbose:\n",
    "            self.trace_func(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "        self.val_loss_min = val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from data import DataGenerator\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu' \n",
    "\n",
    "def save_checkpoint(\n",
    "    model, \n",
    "    optimizer, \n",
    "    e, \n",
    "    loss,\n",
    "    acc, \n",
    "    filename='checkpoint.pth.tar'\n",
    "    ):\n",
    "    print('=> Saving checkpoint')\n",
    "    checkpoint = {     \n",
    "        'state_dict': model.state_dict(),\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "        'epoch': e,\n",
    "        'loss': loss,\n",
    "        'accuracy': acc\n",
    "    }\n",
    "    torch.save(checkpoint, filename)\n",
    "\n",
    "def resume_checkpoint(\n",
    "    model,\n",
    "    optimizer\n",
    "    ):\n",
    "    print('=> Loading checkpoint')\n",
    "    checkpoint = torch.load('checkpoint.pth.tar')\n",
    "    if torch.cuda.is_available():\n",
    "        model.load_state_dict(checkpoint['state_dict'], False)  # define GPU device (cuda[0])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer']) # cuda related code not complete\n",
    "    else:\n",
    "        model.load_state_dict(checkpoint['state_dict'], False)\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    epoch = checkpoint['epoch']+1\n",
    "    loss = checkpoint['loss']\n",
    "    acc = checkpoint['accuracy']\n",
    "    return model, optimizer, epoch, loss, acc\n",
    "    \n",
    "def batch_accuracy(\n",
    "    prediction, # torch.Size([16, 6, 256, 128]\n",
    "    gt # torch.Size([16, 1, 256, 128]\n",
    "    ): \n",
    "    acc = (torch.squeeze(gt.long(), dim=1) == torch.argmax(prediction.long(), dim=1)+1).float().mean().item() \n",
    "    return acc # prediction class indices starts from 0. +1 to match label values. \n",
    "\n",
    "def split_train_test_val(\n",
    "    dataset\n",
    "    ):\n",
    "    len_train = int(0.9*len(dataset))\n",
    "    len_val = int(0.05*len(dataset))\n",
    "    len_test = len(dataset)-len_train-len_val\n",
    "    train_data, val_data, test_data = random_split(dataset, [len_train, len_val, len_test])\n",
    "    return train_data, val_data, test_data\n",
    "\n",
    "def get_loaders(\n",
    "    bs\n",
    "    ):\n",
    "    dataset = DataGenerator()\n",
    "    train_data, val_data, test_data = split_train_test_val(dataset)\n",
    "    train_loader = DataLoader(train_data, batch_size=bs, shuffle=True, pin_memory=True)\n",
    "    val_loader = DataLoader(val_data, batch_size=bs, shuffle=True, pin_memory=True)\n",
    "    test_loader = DataLoader(test_data, batch_size=bs, shuffle=True, pin_memory=True) \n",
    "    torch.save(test_loader, 'test_loader.pth')         \n",
    "    return train_loader, val_loader\n",
    "\n",
    "def save_prediction_as_imgs(\n",
    "    data, # torch.Size([16, 1, 256, 128])\n",
    "    label, # torch.Size([16, 1, 256, 128])\n",
    "    prediction, # torch.Size([16, 6, 256, 128])\n",
    "    e, \n",
    "    ix,\n",
    "    ):\n",
    "    script_dir = os.path.abspath('')\n",
    "    results_dir = os.path.join(script_dir, 'Validation images/')\n",
    "    if not os.path.isdir(results_dir):\n",
    "        os.makedirs(results_dir)\n",
    "    \n",
    "    data, label, prediction = torch.squeeze(data, dim=1), torch.squeeze(label, dim=1), torch.argmax(prediction, dim=1) # torch.Size([16, 256, 128])\n",
    "    prediction = prediction+1 # class indices start from zero. here to get the orignal class naming.\n",
    "    plt.rcParams['xtick.bottom'] = plt.rcParams['xtick.labelbottom'] = False\n",
    "    plt.rcParams['xtick.top'] = plt.rcParams['xtick.labeltop'] = True\n",
    "\n",
    "    for i, (D, L, P) in enumerate(zip(data, label, prediction), 1): \n",
    "        fig, ax = plt.subplots(1, 3, figsize=(20,18))\n",
    "        im0 = ax[0].imshow(D, aspect='auto', cmap='RdBu') # torch.Size([256, 128])\n",
    "        im1 = ax[1].imshow(L, aspect='auto')\n",
    "        im2 = ax[2].imshow(P, aspect='auto')\n",
    "        ax[0].set_title('Image')\n",
    "        ax[1].set_title('Label')\n",
    "        ax[2].set_title('Prediction')\n",
    "        ax[0].set_ylabel('Height')\n",
    "        ax[1].set_xlabel('Width') # loc='center'\n",
    "        cbar0 = fig.colorbar(im0, ax=ax[0], orientation='horizontal', pad=0.02)\n",
    "        cbar1 = fig.colorbar(im1, ax=ax[1], orientation='horizontal', pad=0.02)\n",
    "        cbar2 = fig.colorbar(im2, ax=ax[2], orientation='horizontal', pad=0.02)\n",
    "        cbar0.ax.set_ylabel(r'Amplitude', fontsize=8)\n",
    "        cbar1.ax.set_ylabel(r'Class', fontsize=8)\n",
    "        cbar2.ax.set_ylabel(r'Class', fontsize=8)\n",
    "        plt.suptitle('Validation of random images (Epoch:{}, Batch:{}.{})'.format(e, ix, i))\n",
    "        file_name = 'Epoch_{}_Batch_{}.{}_Validation of random images.png'.format(e, ix, i)\n",
    "        plt.savefig(results_dir + file_name)\n",
    "        #plt.show()\n",
    "        plt.clf() \n",
    "\n",
    "def loss_EarlyStopping_plot( # thanks MNIST_Early_Stopping_example.ipynb\n",
    "    train_loss,\n",
    "    val_loss\n",
    "    ):\n",
    "  \n",
    "    plt.rcParams['xtick.bottom'] = plt.rcParams['xtick.labelbottom'] = True\n",
    "    plt.rcParams['xtick.top'] = plt.rcParams['xtick.labeltop'] = False\n",
    "\n",
    "    # loss\n",
    "    fig = plt.figure(figsize=(10,8))\n",
    "    plt.plot(range(1,len(train_loss)+1),train_loss, label='Training Loss')\n",
    "    plt.plot(range(1,len(val_loss)+1),val_loss,label='Validation Loss')\n",
    "\n",
    "    # min_val_loss\n",
    "    min_val_loss = val_loss.index(min(val_loss))+1 \n",
    "    plt.axvline(min_val_loss, linestyle='--', color='r',label='Early Stopping Checkpoint')\n",
    "\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.ylim(0, 0.5) # consistent scale\n",
    "    plt.xlim(0, len(train_loss)+1) # consistent scale\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    #plt.show()\n",
    "    plt.savefig('loss_EarlyStopping_plot.png', bbox_inches='tight')\n",
    "\n",
    "def loss_acc_plot( # thanks Rath S. R. (2021). Saving and Loading the Best Model in PyTorch. Debugger cafe.\n",
    "    train_loss, \n",
    "    train_acc, \n",
    "    val_loss, \n",
    "    val_acc\n",
    "    ):\n",
    "    # accuracy plots\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    plt.plot(\n",
    "        train_acc, color='green', linestyle='-', \n",
    "        label='train accuracy'\n",
    "    )\n",
    "    plt.plot(\n",
    "        val_acc, color='blue', linestyle='-', \n",
    "        label='validation accuracy'\n",
    "    )\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig('Accuracy.png')\n",
    "    \n",
    "    # loss plots\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    plt.plot(\n",
    "        train_loss, color='orange', linestyle='-', \n",
    "        label='train loss'\n",
    "    )\n",
    "    plt.plot(\n",
    "        val_loss, color='red', linestyle='-', \n",
    "        label='validation loss'\n",
    "    )\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig('Loss.png')\n",
    "\n",
    "def save_model(\n",
    "    model\n",
    "    ):\n",
    "    print('Completed experiment \\n'\n",
    "            '=> Saving model')\n",
    "    torch.save(model.state_dict(), 'complete_trained_model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from utils import (save_checkpoint, resume_checkpoint, batch_accuracy, get_loaders, save_prediction_as_imgs, loss_EarlyStopping_plot, loss_acc_plot, save_model)\n",
    "#from early-stopping import EarlyStopping\n",
    "#from model import UNET\n",
    "import torch\n",
    "import torch.cuda\n",
    "import torch.cuda.amp\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import wandb\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Assign utils and constant\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu' \n",
    "\n",
    "def train_fn(model, train_loader, criterion, optimizer, scaler, epoch):\n",
    "\n",
    "    train_losses, avg_train_losses, train_accs, avg_train_accs = [], [], [], []  \n",
    "    train_loop = tqdm(train_loader)\n",
    "    model.train()   \n",
    "    for ix, (data, label) in enumerate(train_loop, 1):\n",
    " \n",
    "        if torch.cuda.is_available(): # cuda related code not complete\n",
    "            data, label = data, label\n",
    "                \n",
    "            # forward propagation\n",
    "            with torch.cuda.amp.autocast():\n",
    "                prediction = model(data)\n",
    "                loss = criterion(prediction, torch.squeeze(label.type(torch.LongTensor)-1, dim=1)) \n",
    "\n",
    "        else:\n",
    "            data, label = data, label # torch.Size([16, 1, 256, 128]\n",
    "            prediction = model(data)\n",
    "            loss = criterion(prediction, torch.squeeze(label.type(torch.LongTensor)-1, dim=1)) # CrossEntropy() accepts [N, W, H] and class indices starting 0\n",
    "            acc = batch_accuracy(prediction, label)\n",
    "            \n",
    "        # tracker, wandb\n",
    "        train_losses.append(loss.item()), train_accs.append(acc)\n",
    "        wandb.log({'train_loss': loss, 'train_accuracy': acc, 'epoch': epoch, 'batch': ix})\n",
    "        wandb.watch(model)\n",
    "          \n",
    "        # backward propagation\n",
    "        optimizer.zero_grad()\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "  \n",
    "        # update tqdm loop / loss in forward propagation\n",
    "        train_loop.set_postfix({'train_loss':loss.item(), 'train_acc':acc}) \n",
    "    \n",
    "    train_loss, train_acc = np.average(train_losses), np.average(train_accs)\n",
    "    avg_train_losses.append(train_loss), avg_train_accs.append(train_acc)\n",
    "\n",
    "    return model, optimizer, avg_train_losses, avg_train_accs\n",
    "\n",
    "def val_fn(val_loader, model, criterion, epoch, early_stopping):\n",
    "\n",
    "    val_loop = tqdm(val_loader)\n",
    "    val_losses, avg_val_losses, val_accs, avg_val_accs = [], [], [], []\n",
    "    model.eval() \n",
    "\n",
    "    with torch.no_grad():    \n",
    "        for ix, (data, label) in enumerate(val_loop, 1):\n",
    "            if torch.cuda.is_available(): # cuda related code are not complete\n",
    "                data, label = data, label \n",
    "                \n",
    "            else:\n",
    "                data, label = data, label # torch.Size([16, 1, 256, 128])\n",
    "                \n",
    "            # forward pass\n",
    "            prediction = model(data)\n",
    "            loss = criterion(prediction, torch.squeeze(label.type(torch.LongTensor)-1, dim=1)) # CrossEntropy() accepts [N, W, H] and class indices starting 0\n",
    "            acc = batch_accuracy(prediction, label)\n",
    "\n",
    "            # loss accuracy tracker, tqdm, wandb, utils, EarlyStopping\n",
    "            val_losses.append(loss.item()), val_accs.append(acc)\n",
    "            val_loop.set_postfix({'val_loss':loss.item(), 'val_acc':acc})\n",
    "            wandb.log({'val_loss': loss, 'val_accuracy': acc, 'epoch': epoch, 'batch': ix})\n",
    "            # wandb.log({\n",
    "            #     'confusion_matrix': \n",
    "            #         wandb.plot.confusion_matrix( \n",
    "            #             preds = torch.argmax(prediction, dim=1),\n",
    "            #             y_true = torch.squeeze(label.long, dim=1),\n",
    "            #             class_names = ['Class 1', 'Class 2', 'Class 3', 'Class 4', 'Class 5', 'Class 6']),\n",
    "            #     })\n",
    "            save_prediction_as_imgs(data, label, prediction, epoch, ix)\n",
    "         \n",
    "        val_loss, val_acc = np.average(val_losses), np.average(val_accs)\n",
    "        avg_val_losses.append(val_loss), avg_val_accs.append(val_acc)\n",
    "\n",
    "        early_stopping(val_loss, model)\n",
    "        if early_stopping.early_stop:\n",
    "            print('=> Early stopping')\n",
    "            \n",
    "    return avg_val_losses, avg_val_accs\n",
    "\n",
    "def train(model, lr, epoch, bs, criterion, optimizer, scaler, patience, load_checkpoint, entity, project):\n",
    "\n",
    "    # wandb, EarlyStopping, loss & accuracy tracker\n",
    "    wandb.init(project=project, entity=entity)\n",
    "    wandb.config = {\n",
    "        'learning_rate': lr,\n",
    "        'epochs': epoch,\n",
    "        'batch_size': bs\n",
    "    }\n",
    "    early_stopping = EarlyStopping(patience=patience, verbose=True, path='EarlyStopping_checkpoint.pth')\n",
    "    train_loss, val_loss, train_acc, val_acc = [], [], [], []\n",
    "\n",
    "    # assign the defined paramteres and hyperparameters\n",
    "    model = eval(model)(in_channels=1, out_channels=6).to(device)\n",
    "    criterion = getattr(nn, criterion)()\n",
    "    optimizer = getattr(optim, optimizer)(model.parameters(), lr=lr)\n",
    "    scaler = getattr(torch.cuda.amp, scaler)()\n",
    "    train_loader, val_loader = get_loaders(bs=bs)\n",
    "\n",
    "    # resume checkpoint of partially trained model\n",
    "    if load_checkpoint:\n",
    "        model, optimizer, resume_epoch, resume_train_loss, resume_train_acc = resume_checkpoint(model, optimizer) \n",
    "        avg_val_loss, avg_val_acc = val_fn(val_loader, model, criterion, resume_epoch-1, early_stopping)\n",
    "        train_loss.append(resume_train_loss.item()), train_acc.append(resume_train_acc.item()), val_loss.append(avg_val_loss.pop()), val_acc.append(avg_val_acc.pop()) # resume_train_loss: torch.size([]), val_loss: (1, 0) <class 'list'>\n",
    "        \n",
    "        for e in range(resume_epoch, epoch+1):\n",
    "            model, optimizer, avg_train_loss, avg_train_acc = train_fn(model, train_loader, criterion, optimizer, scaler, e)\n",
    "            train_loss.append(avg_train_loss.pop()), train_acc.append(avg_train_acc.pop())\n",
    "            save_checkpoint(model, optimizer, e, train_loss, train_acc)\n",
    "            avg_val_loss, avg_val_acc = val_fn(val_loader, model, criterion, e, early_stopping)\n",
    "            val_loss.append(avg_val_loss.pop()), val_acc.append(avg_val_acc.pop())\n",
    "\n",
    "    else:\n",
    "        for e in range(1, epoch+1):\n",
    "            model, optimizer, avg_train_loss, avg_train_acc = train_fn(model, train_loader, criterion, optimizer, scaler, e)\n",
    "            train_loss.append(avg_train_loss.pop()), train_acc.append(avg_train_acc.pop())\n",
    "            save_checkpoint(model, optimizer, e, train_loss, train_acc)\n",
    "            avg_val_loss, avg_val_acc = val_fn(val_loader, model, criterion, e, early_stopping)\n",
    "            val_loss.append(avg_val_loss.pop()), val_acc.append(avg_val_acc.pop())\n",
    "    \n",
    "    loss_EarlyStopping_plot(train_loss, val_loss), loss_acc_plot(train_loss, train_acc, val_loss, val_acc)\n",
    "    save_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from train import train\n",
    "\n",
    "load_checkpoint = False # not accurate to resume loaders\n",
    "\n",
    "# hyperparameters / model parameters\n",
    "lr =  1e-3\n",
    "bs = 16\n",
    "epoch = 10\n",
    "model = 'UNET'\n",
    "criterion = 'CrossEntropyLoss' \n",
    "optimizer = 'Adam' #SGD, Adagrad\n",
    "scaler = 'GradScaler'\n",
    "patience = 3\n",
    "\n",
    "# wandb configurations\n",
    "entity = 'your_name' # insert wandb username\n",
    "project = 'your_project_name' # insert wandb project name\n",
    "\n",
    "def main():   \n",
    "        train(\n",
    "                model=model,\n",
    "                lr=lr,\n",
    "                epoch=epoch,\n",
    "                bs=bs,\n",
    "                criterion=criterion,\n",
    "                optimizer=optimizer,\n",
    "                scaler=scaler,\n",
    "                patience=patience,\n",
    "                load_checkpoint=load_checkpoint,\n",
    "                entity=entity,\n",
    "                project=project\n",
    "        )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bf8e42502e00c8f792b37f407ecfa39061bb58f0e5225c1a6b4c9a02746ef4ba"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
